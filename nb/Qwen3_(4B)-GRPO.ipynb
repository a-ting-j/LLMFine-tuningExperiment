{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a-ting-j/LLMFine-tuningExperiment/blob/main/nb/Qwen3_(4B)-GRPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tV2TmZmyuXMr"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
        "</div>\n",
        "\n",
        "To install Unsloth your local device, follow [our guide](https://docs.unsloth.ai/get-started/install-and-update). This notebook is licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUABefNcuXMr"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmXztjpvuXMs"
      },
      "source": [
        "\n",
        "Introducing FP8 precision training for faster RL inference. [Read Blog](https://docs.unsloth.ai/new/fp8-reinforcement-learning).\n",
        "\n",
        "Unsloth's [Docker image](https://hub.docker.com/r/unsloth/unsloth) is here! Start training with no setup & environment issues. [Read our Guide](https://docs.unsloth.ai/new/how-to-train-llms-with-unsloth-and-docker).\n",
        "\n",
        "[gpt-oss RL](https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning) is now supported with the fastest inference & lowest VRAM. Try our [new notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-GRPO.ipynb) which creates kernels!\n",
        "\n",
        "Introducing [Vision](https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl) and [Standby](https://docs.unsloth.ai/basics/memory-efficient-rl) for RL! Train Qwen, Gemma etc. VLMs with GSPO - even faster with less VRAM.\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlmr3DutuXMs"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CbNEfO2kuXMs"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "os.environ[\"UNSLOTH_VLLM_STANDBY\"] = \"1\" # [NEW] Extra 30% context lengths!\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    # If you're not in Colab, just use pip install or uv pip install\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    pass # For Colab / Kaggle, we need extra instructions hidden below \\/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0YvNHCCZuXMs"
      },
      "outputs": [],
      "source": [
        "#@title Colab Extra Install { display-mode: \"form\" }\n",
        "%%capture\n",
        "import os\n",
        "!pip install --upgrade -qqq uv\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    # If you're not in Colab, just use pip install!\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    try: import numpy, PIL; get_numpy = f\"numpy=={numpy.__version__}\"; get_pil = f\"pillow=={PIL.__version__}\"\n",
        "    except: get_numpy = \"numpy\"; get_pil = \"pillow\"\n",
        "    try: import subprocess; is_t4 = \"Tesla T4\" in str(subprocess.check_output([\"nvidia-smi\"]))\n",
        "    except: is_t4 = False\n",
        "    get_vllm, get_triton = (\"vllm==0.9.2\", \"triton==3.2.0\") if is_t4 else (\"vllm==0.10.2\", \"triton\")\n",
        "    !uv pip install -qqq --upgrade \\\n",
        "        unsloth {get_vllm} {get_numpy} {get_pil} torchvision bitsandbytes xformers\n",
        "    !uv pip install -qqq {get_triton}\n",
        "!uv pip install transformers==4.56.2\n",
        "!uv pip install --no-deps trl==0.22.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkH_y8UC9lvv"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jN75nmdx9lvw"
      },
      "source": [
        "Goal: To convert `Qwen3-4B-Base` into a reasoning model via GRPO by using OpenR1's Math dataset.\n",
        "\n",
        "We first pre fine-tune the model to make GRPO skip trying to match formatting - this speeds GRPO up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5de75c9abdf34a7687cced18e1a61f61",
            "04405b9942e74d24b3fe31d771559a49",
            "00919cc8d24e47ed966ff27b9f70a2e7",
            "155dfa5fd685467687a1526891846a22",
            "422624a5bdc54d98b543439bc096d941",
            "23607694d71145339e8aa231d48f63c0",
            "10ff7194dd514aa094eab5041776882f",
            "9ba0ba8a8f54484a9267a042d1c62146",
            "6b2b8be2edb349cd80e862429bcd5fe9",
            "d102db560b904f2b938afafc880e40fb",
            "f63f09d55e8346b7a2644112358d7fa7",
            "bab2ffabb7d6482ca9495950f537619f",
            "bf1145ec6de34f5cbc81199b3bd82fc0",
            "6f67a5ea89e943deb0f64b0a0b52874b",
            "9648dbd02fe04eb79d51ade0f9a95cca",
            "a3cdc6577017430b838f8e3368ea29f8",
            "ff9bc85eb2f647c7b060c140510d892b",
            "a1f44ad66ecb4cb2b1f47993669f422b",
            "b1d7ef98b84847c5a2ea0edbc85ab9a5",
            "ded0d4c78da6448f84e4cf49c65e2db7",
            "e8870da57aeb49379fc2ffd3ffbe3c13",
            "7861a0bed9e84c71934d7347ebb7a50b"
          ]
        },
        "id": "DkIvEkIIkEyB",
        "outputId": "ed3f7037-cc86-41bb-8c58-21db5b9cf267"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "INFO 12-22 09:27:50 [__init__.py:244] Automatically detected platform cuda.\n",
            "ERROR 12-22 09:27:54 [fa_utils.py:57] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "INFO 12-22 09:28:03 [vllm_utils.py:702] Unsloth: Patching vLLM v1 graph capture\n",
            "INFO 12-22 09:28:03 [vllm_utils.py:732] Unsloth: Patching vLLM v0 graph capture\n",
            "==((====))==  Unsloth 2025.12.8: Fast Llama patching. Transformers: 4.56.2. vLLM: 0.9.2.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: Standby mode is enabled. Changing `gpu_memory_utilization` to 0.9.\n",
            "Unsloth: vLLM loading unsloth/Llama-3.2-1B-Instruct with actual GPU utilization = 89.15%\n",
            "Unsloth: Your GPU has CUDA compute capability 7.5 with VRAM = 14.74 GB.\n",
            "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2048. Num Sequences = 48.\n",
            "Unsloth: vLLM's KV Cache can use up to 10.8 GB. Also swap space = 0 GB.\n",
            "Unsloth: `cudagraph_mode` is not in `from vllm.config import CompilationConfig`\n",
            "Unsloth: Disabling `disable_cascade_attn` in vLLM to allow for better on policy RL!\n",
            "INFO 12-22 09:28:29 [config.py:841] This model supports multiple tasks: {'embed', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 12-22 09:28:29 [config.py:3371] Casting torch.bfloat16 to torch.float16.\n",
            "INFO 12-22 09:28:29 [config.py:1472] Using max model len 2048\n",
            "WARNING 12-22 09:28:29 [arg_utils.py:1735] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n",
            "INFO 12-22 09:28:31 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=4096.\n",
            "INFO 12-22 09:28:31 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.2) with config: model='unsloth/Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='unsloth/Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Llama-3.2-1B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":4,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":true,\"max_capture_size\":48,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
            "INFO 12-22 09:28:33 [cuda.py:311] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
            "INFO 12-22 09:28:33 [cuda.py:360] Using XFormers backend.\n",
            "INFO 12-22 09:28:34 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "INFO 12-22 09:28:34 [model_runner.py:1171] Starting to load model unsloth/Llama-3.2-1B-Instruct...\n",
            "INFO 12-22 09:28:35 [weight_utils.py:292] Using model weights format ['*.safetensors']\n",
            "INFO 12-22 09:28:36 [weight_utils.py:345] No model.safetensors.index.json found in remote.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5de75c9abdf34a7687cced18e1a61f61"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 12-22 09:28:44 [default_loader.py:272] Loading weights took 8.59 seconds\n",
            "INFO 12-22 09:28:44 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
            "INFO 12-22 09:28:45 [model_runner.py:1203] Model loading took 2.3783 GiB and 10.157267 seconds\n",
            "INFO 12-22 09:28:49 [worker.py:294] Memory profiling takes 3.29 seconds\n",
            "INFO 12-22 09:28:49 [worker.py:294] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.89) = 13.14GiB\n",
            "INFO 12-22 09:28:49 [worker.py:294] model weights take 2.38GiB; non_torch_memory takes 0.03GiB; PyTorch activation peak memory takes 0.31GiB; the rest of the memory reserved for KV Cache is 10.42GiB.\n",
            "INFO 12-22 09:28:50 [executor_base.py:113] # cuda blocks: 21338, # CPU blocks: 0\n",
            "INFO 12-22 09:28:50 [executor_base.py:118] Maximum concurrency for 2048 tokens per request: 166.70x\n",
            "INFO 12-22 09:28:50 [vllm_utils.py:737] Unsloth: Running patched vLLM v0 `capture_model`.\n",
            "INFO 12-22 09:28:50 [model_runner.py:1513] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Capturing CUDA graph shapes:   0%|          | 0/9 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bab2ffabb7d6482ca9495950f537619f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 12-22 09:28:52 [model_runner.py:1671] Graph capturing finished in 2 secs, took 0.09 GiB\n",
            "INFO 12-22 09:28:52 [vllm_utils.py:744] Unsloth: Patched vLLM v0 graph capture finished in 2 secs.\n",
            "INFO 12-22 09:28:53 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 7.47 seconds\n",
            "Unsloth: Standby mode is enabled. Pre-sleeping vLLM model to reduce OOMs.\n",
            "Unsloth: Just some info: will skip parsing ['post_feedforward_layernorm', 'post_attention_layernorm', 'input_layernorm', 'k_norm', 'post_layernorm', 'norm', 'attention_norm', 'norm2', 'layer_norm1', 'q_norm', 'layer_norm2', 'ffn_norm', 'norm1', 'pre_feedforward_layernorm']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at unsloth/Llama-3.2-1B-Instruct and are newly initialized: ['lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing substitution for additional_keys=set()\n",
            "Unsloth: Just some info: will skip parsing ['post_feedforward_layernorm', 'post_attention_layernorm', 'input_layernorm', 'k_norm', 'post_layernorm', 'norm', 'cross_attn_post_attention_layernorm', 'attention_norm', 'norm2', 'layer_norm1', 'q_norm', 'cross_attn_input_layernorm', 'layer_norm2', 'ffn_norm', 'norm1', 'pre_feedforward_layernorm']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.12.8 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Can increase for longer reasoning traces\n",
        "lora_rank = 32 # Larger rank = smarter, but slower\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = False, # False for LoRA 16bit\n",
        "    fast_inference = True, # Enable vLLM fast inference\n",
        "    max_lora_rank = lora_rank,\n",
        "    gpu_memory_utilization = 0.9, # Reduce if out of memory\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha = lora_rank*2, # *2 speeds up training\n",
        "    use_gradient_checkpointing = \"unsloth\", # Reduces memory usage\n",
        "    random_state = 3407,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9DuiVRLhMco"
      },
      "source": [
        "### GRPO chat template\n",
        "Since we're using a base model, we should set a chat template. You can make your own chat template as well!\n",
        "1. DeepSeek uses `<think>` and `</think>`, but this is **not** necessary - you can customize it however you like!\n",
        "2. A `system_prompt` is recommended to at least guide the model's responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "6UjowCbT-cFz",
        "outputId": "7e0f9768-9f3e-48fd-f716-ff66801ad6b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You are given a problem.\\nThink about the problem and provide your working out.\\nPlace it between <start_working_out> and <end_working_out>.\\nThen, provide your solution between <SOLUTION></SOLUTION>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "reasoning_start = \"<start_working_out>\" # Acts as <think>\n",
        "reasoning_end   = \"<end_working_out>\"   # Acts as </think>\n",
        "solution_start  = \"<SOLUTION>\"\n",
        "solution_end    = \"</SOLUTION>\"\n",
        "\n",
        "system_prompt = \\\n",
        "f\"\"\"You are given a problem.\n",
        "Think about the problem and provide your working out.\n",
        "Place it between {reasoning_start} and {reasoning_end}.\n",
        "Then, provide your solution between {solution_start}{solution_end}\"\"\"\n",
        "system_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGgs0MJkDkYL"
      },
      "source": [
        "We create a simple chat template below. Notice `add_generation_prompt` includes prepending `<start_working_out>` to guide the model to start its reasoning process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Y3fF9gMujY02"
      },
      "outputs": [],
      "source": [
        "chat_template = \\\n",
        "    \"{% if messages[0]['role'] == 'system' %}\"\\\n",
        "        \"{{ messages[0]['content'] + eos_token }}\"\\\n",
        "        \"{% set loop_messages = messages[1:] %}\"\\\n",
        "    \"{% else %}\"\\\n",
        "        \"{{ '{system_prompt}' + eos_token }}\"\\\n",
        "        \"{% set loop_messages = messages %}\"\\\n",
        "    \"{% endif %}\"\\\n",
        "    \"{% for message in loop_messages %}\"\\\n",
        "        \"{% if message['role'] == 'user' %}\"\\\n",
        "            \"{{ message['content'] }}\"\\\n",
        "        \"{% elif message['role'] == 'assistant' %}\"\\\n",
        "            \"{{ message['content'] + eos_token }}\"\\\n",
        "        \"{% endif %}\"\\\n",
        "    \"{% endfor %}\"\\\n",
        "    \"{% if add_generation_prompt %}{{ '{reasoning_start}' }}\"\\\n",
        "    \"{% endif %}\"\n",
        "\n",
        "# Replace with out specific template:\n",
        "chat_template = chat_template\\\n",
        "    .replace(\"'{system_prompt}'\",   f\"'{system_prompt}'\")\\\n",
        "    .replace(\"'{reasoning_start}'\", f\"'{reasoning_start}'\")\n",
        "tokenizer.chat_template = chat_template"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEcLdymBEHdk"
      },
      "source": [
        "Let's see how our chat template behaves on an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "BciEDYSSYFNj",
        "outputId": "e809f778-6b32-4bdd-a1b5-a8deb42c620c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"You are given a problem.\\nThink about the problem and provide your working out.\\nPlace it between <start_working_out> and <end_working_out>.\\nThen, provide your solution between <SOLUTION></SOLUTION><|eot_id|>What is 1+1?<start_working_out>I think it's 2.<end_working_out><SOLUTION>2</SOLUTION><|eot_id|>What is 2+2?<start_working_out>\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "tokenizer.apply_chat_template([\n",
        "    {\"role\" : \"user\", \"content\" : \"What is 1+1?\"},\n",
        "    {\"role\" : \"assistant\", \"content\" : f\"{reasoning_start}I think it's 2.{reasoning_end}{solution_start}2{solution_end}\"},\n",
        "    {\"role\" : \"user\", \"content\" : \"What is 2+2?\"},\n",
        "], tokenize = False, add_generation_prompt = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mdsuGjxHrjT"
      },
      "source": [
        "### Pre fine-tuning for formatting\n",
        "We now use a subset of NVIDIA's [Open Math Reasoning dataset](https://huggingface.co/datasets/nvidia/OpenMathReasoning) which was filtered to only include high quality DeepSeek R1 traces.\n",
        "\n",
        "We'll only filter ~59 or so examples to first \"prime\" / pre fine-tune the model to understand our custom GRPO formatting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "AXxM2lStVIkd",
        "outputId": "45ceba60-1a16-45c1-d98f-500c7ea8ea7a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      expected_answer                                            problem  \\\n",
              "0                  14  Given $\\sqrt{x^2+165}-\\sqrt{x^2-52}=7$ and $x$...   \n",
              "6                  -2  Find the value of the parameter $a$ for which ...   \n",
              "9                  18  What is the sum of all real numbers $x$ for wh...   \n",
              "13                  2  Evaluate the sum \\(\\sum_{n=1}^\\infty \\frac{\\ph...   \n",
              "17                 30  What is the largest positive integer that divi...   \n",
              "...               ...                                                ...   \n",
              "19243             244  Let \\( p \\), \\( q \\), and \\( r \\) be the disti...   \n",
              "19245               1  A bug is on the $0$ of a number line. At any p...   \n",
              "19247               4  A bus left point X for point Y. Two hours late...   \n",
              "19248              18  Each interior angle of a regular n-gon measure...   \n",
              "19250          0.8960  Find the probability that the second blue resu...   \n",
              "\n",
              "                                      generated_solution  \n",
              "0      <think>\\nOkay, let's see. I need to solve the ...  \n",
              "6      <think>\\nOkay, so I need to find the value of ...  \n",
              "9      <think>\\nOkay, so I need to solve the equation...  \n",
              "13     <think>\\nOkay, so I need to evaluate the infin...  \n",
              "17     <think>\\nAlright, so I need to find the larges...  \n",
              "...                                                  ...  \n",
              "19243  <think>\\nOkay, so I need to find the value of ...  \n",
              "19245  <think>\\nOkay, so I have this problem where a ...  \n",
              "19247  <think>\\nOkay, let's tackle this problem step ...  \n",
              "19248  <think>\\nOkay, let's see. I need to find the n...  \n",
              "19250  <think>\\nOkay, so I need to find the probabili...  \n",
              "\n",
              "[7507 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-29ef2832-38a2-40d9-8455-5159a8632d1c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>expected_answer</th>\n",
              "      <th>problem</th>\n",
              "      <th>generated_solution</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>14</td>\n",
              "      <td>Given $\\sqrt{x^2+165}-\\sqrt{x^2-52}=7$ and $x$...</td>\n",
              "      <td>&lt;think&gt;\\nOkay, let's see. I need to solve the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-2</td>\n",
              "      <td>Find the value of the parameter $a$ for which ...</td>\n",
              "      <td>&lt;think&gt;\\nOkay, so I need to find the value of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>18</td>\n",
              "      <td>What is the sum of all real numbers $x$ for wh...</td>\n",
              "      <td>&lt;think&gt;\\nOkay, so I need to solve the equation...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2</td>\n",
              "      <td>Evaluate the sum \\(\\sum_{n=1}^\\infty \\frac{\\ph...</td>\n",
              "      <td>&lt;think&gt;\\nOkay, so I need to evaluate the infin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>30</td>\n",
              "      <td>What is the largest positive integer that divi...</td>\n",
              "      <td>&lt;think&gt;\\nAlright, so I need to find the larges...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19243</th>\n",
              "      <td>244</td>\n",
              "      <td>Let \\( p \\), \\( q \\), and \\( r \\) be the disti...</td>\n",
              "      <td>&lt;think&gt;\\nOkay, so I need to find the value of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19245</th>\n",
              "      <td>1</td>\n",
              "      <td>A bug is on the $0$ of a number line. At any p...</td>\n",
              "      <td>&lt;think&gt;\\nOkay, so I have this problem where a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19247</th>\n",
              "      <td>4</td>\n",
              "      <td>A bus left point X for point Y. Two hours late...</td>\n",
              "      <td>&lt;think&gt;\\nOkay, let's tackle this problem step ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19248</th>\n",
              "      <td>18</td>\n",
              "      <td>Each interior angle of a regular n-gon measure...</td>\n",
              "      <td>&lt;think&gt;\\nOkay, let's see. I need to find the n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19250</th>\n",
              "      <td>0.8960</td>\n",
              "      <td>Find the probability that the second blue resu...</td>\n",
              "      <td>&lt;think&gt;\\nOkay, so I need to find the probabili...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7507 rows √ó 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-29ef2832-38a2-40d9-8455-5159a8632d1c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-29ef2832-38a2-40d9-8455-5159a8632d1c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-29ef2832-38a2-40d9-8455-5159a8632d1c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e1febed3-86ef-4b7e-be4b-92d4c47b1d88\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e1febed3-86ef-4b7e-be4b-92d4c47b1d88')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e1febed3-86ef-4b7e-be4b-92d4c47b1d88 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "dataset",
              "summary": "{\n  \"name\": \"dataset\",\n  \"rows\": 7507,\n  \"fields\": [\n    {\n      \"column\": \"expected_answer\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 868,\n        \"samples\": [\n          \"672\",\n          \"335\",\n          \"575757\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"problem\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3895,\n        \"samples\": [\n          \"A club with 7 members forms three-person committees, but no two committees can have more than one member in common. What is the maximum number of committees that can be formed?\",\n          \"Find the smallest integer \\\\( a > 2 \\\\) such that \\\\( 2 \\\\mid a \\\\), \\\\( 3 \\\\mid (a+1) \\\\), \\\\( 4 \\\\mid (a+2) \\\\), \\\\( 5 \\\\mid (a+3) \\\\), and \\\\( 6 \\\\mid (a+4) \\\\).\",\n          \"Given the polynomial equation \\\\(x^3 - x = -1\\\\) with roots \\\\(a\\\\), \\\\(b\\\\), and \\\\(c\\\\), find the value of \\\\(\\\\frac{1}{1+a} + \\\\frac{1}{1+b} + \\\\frac{1}{1+c}\\\\).\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"generated_solution\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7507,\n        \"samples\": [\n          \"<think>\\nOkay, let's see. I need to solve this problem where there are three prime numbers p, q, and r. The equations given are pq + qr + rp = 191 and p + q = r - 1. The goal is to find p + q + r. Hmm, primes, so they must be 2 or odd primes. Let me start by analyzing the problem step by step.\\n\\nFirst, the equation p + q = r - 1. If I can express r in terms of p and q, maybe I can substitute that into the first equation. Let's try that. So, from the second equation, r = p + q + 1. That seems straightforward. Now, substituting this into the first equation, we get pq + qr + rp = 191. Let's replace r with (p + q + 1).\\n\\nSo, substituting, the first equation becomes:\\n\\npq + q(p + q + 1) + p(p + q + 1) = 191.\\n\\nLet me expand each term:\\n\\nFirst term: pq.\\n\\nSecond term: q*(p + q + 1) = pq + q\\u00b2 + q.\\n\\nThird term: p*(p + q + 1) = p\\u00b2 + pq + p.\\n\\nNow, adding all these together:\\n\\npq + (pq + q\\u00b2 + q) + (p\\u00b2 + pq + p) = 191.\\n\\nCombine like terms:\\n\\npq + pq + pq = 3pq.\\n\\nq\\u00b2 + p\\u00b2.\\n\\nq + p.\\n\\nSo altogether, the equation becomes:\\n\\np\\u00b2 + q\\u00b2 + 3pq + p + q = 191.\\n\\nHmm. Let me note that.\\n\\nAlternatively, maybe I can factor this expression. Let me see. Let's try to group terms. Wait, another approach: since r = p + q + 1, then p + q + r = (p + q) + (p + q + 1) = 2(p + q) + 1. So if I can find p + q, then multiplying by 2 and adding 1 gives me the answer. That's useful. So perhaps instead of dealing with the first equation directly, I can express it in terms of p + q.\\n\\nBut maybe let's work with the equation we derived: p\\u00b2 + q\\u00b2 + 3pq + p + q = 191.\\n\\nWait, another thought: p\\u00b2 + q\\u00b2 + 3pq. Let's recall that (p + q)^2 = p\\u00b2 + 2pq + q\\u00b2, so p\\u00b2 + q\\u00b2 + 3pq = (p + q)^2 + pq.\\n\\nTherefore, the equation can be rewritten as:\\n\\n(p + q)^2 + pq + p + q = 191.\\n\\nLet me set S = p + q, and P = pq. Then the equation becomes:\\n\\nS\\u00b2 + P + S = 191.\\n\\nBut from the second equation, we know that r = S + 1, so since we need to find S + r = S + (S + 1) = 2S + 1. So our target is 2S + 1. So if we can find S, we can find the answer.\\n\\nNow, the equation is S\\u00b2 + P + S = 191. But S and P are related as S = p + q, P = pq. For two primes p and q, their sum and product. Since p and q are primes, maybe we can list possible primes that add up to S and multiply to P. However, since S and P are variables here, maybe we can express P in terms of S from the equation.\\n\\nFrom S\\u00b2 + P + S = 191, so P = 191 - S\\u00b2 - S.\\n\\nTherefore, P = -S\\u00b2 - S + 191. But P = pq must be positive, so -S\\u00b2 - S + 191 > 0. Therefore, S\\u00b2 + S < 191. Let's see the possible values of S. Since S is the sum of two primes, which are at least 2 each, so S is at least 2 + 2 = 4. Also, since S\\u00b2 + S < 191, let's solve S\\u00b2 + S - 191 < 0. Let's find the roots of S\\u00b2 + S - 191 = 0.\\n\\nUsing quadratic formula: S = [-1 \\u00b1 sqrt(1 + 4*191)] / 2 = [-1 \\u00b1 sqrt(765)] / 2. sqrt(765) is approx 27.66, so S \\u2248 (-1 + 27.66)/2 \\u2248 13.33. So the positive root is approximately 13.33, so S must be less than 13.33. Therefore, S can be integers from 4 up to 13.\\n\\nSo possible S values: 4,5,6,7,8,9,10,11,12,13.\\n\\nBut since p and q are primes, their sum S must be even or odd. Since except for 2, all primes are odd. So if both p and q are odd primes, their sum is even. If one is 2 and the other is odd, then their sum is odd. So S can be even or odd. So possible values of S (from 4 to 13) can be checked.\\n\\nBut maybe instead of all possible S, let's check possible S values from 4 to 13 and see if for each S, P = 191 - S\\u00b2 - S, and check if P can be expressed as the product of two primes that add up to S.\\n\\nAlternatively, maybe first check which S gives P as a product of two primes.\\n\\nLet's start with S=4. Then P = 191 - 16 -4 = 171. Then check if 171 can be written as product of two primes that add up to 4. But 4 is the sum. The primes could be 2 and 2 (since 2+2=4). Then 2*2=4, but P here is 171. 4 \\u2260 171, so S=4 is invalid.\\n\\nNext S=5: P=191 -25 -5=161. 161 factors into 7*23. Check if 7 + 23 = 30, which is not 5. So that's not possible. Alternatively, are there primes adding to 5? 2 and 3, since 2+3=5. Then P=2*3=6. But 6 \\u2260 161. So S=5 invalid.\\n\\nS=6: P=191 -36 -6=149. 149 is a prime number, so can't be expressed as product of two primes. So invalid.\\n\\nS=7: P=191 -49 -7=135. 135 factors into 5*27 (but 27 not prime), 3*45, 9*15, none primes. So no. So invalid.\\n\\nS=8: P=191 -64 -8=119. 119 factors into 7*17. Check if 7+17=24\\u22608. Primes adding to 8 are 3+5=8 or 5+3. Then P=15. But 15\\u2260119. So invalid.\\n\\nS=9: P=191 -81 -9=101. 101 is prime, so no.\\n\\nS=10: P=191 -100 -10=81. 81=9*9, but 9 not prime. So no.\\n\\nS=11: P=191 -121 -11=59. 59 is prime. So no.\\n\\nS=12: P=191 -144 -12=35. 35=5*7. Check if 5 +7=12? Yes! 5 +7=12. So here, S=12, which is p + q=12, and pq=35. 5 and 7 are primes. So this works.\\n\\nSo then p=5 and q=7, or p=7 and q=5. Then r = S + 1 =12 +1=13. Check if r is prime: 13 is prime. So yes. So then p, q, r are 5,7,13 or 7,5,13. Then p + q + r =5 +7 +13=25. So 25 would be the answer.\\n\\nWait, let me check S=13 as well just to be thorough. S=13: P=191 -169 -13=9. 9=3*3. Check if 3 +3=6\\u226013. So no. So no.\\n\\nTherefore, only S=12 gives valid primes. So the answer is 25.\\n\\nLet me verify the original equations. pq + qr + rp. Let p=5, q=7, r=13. Then 5*7 +7*13 +13*5 =35 +91 +65=35+91=126+65=191. Which matches. And p + q =5 +7=12. r -1=13 -1=12. So that also matches. So it's correct. Therefore, the answer is 25.\\n</think>To solve the problem where three prime numbers \\\\( p, q, \\\\) and \\\\( r \\\\) satisfy the equations \\\\( pq + qr + rp = 191 \\\\) and \\\\( p + q = r - 1 \\\\), we proceed as follows:\\n\\n1. **Express \\\\( r \\\\) in terms of \\\\( p \\\\) and \\\\( q \\\\):**\\n   From the equation \\\\( p + q = r - 1 \\\\), we can solve for \\\\( r \\\\):\\n   \\\\[\\n   r = p + q + 1\\n   \\\\]\\n\\n2. **Substitute \\\\( r \\\\) into the first equation:**\\n   Substitute \\\\( r = p + q + 1 \\\\) into the equation \\\\( pq + qr + rp = 191 \\\\):\\n   \\\\[\\n   pq + q(p + q + 1) + p(p + q + 1) = 191\\n   \\\\]\\n   Expanding and combining like terms:\\n   \\\\[\\n   pq + pq + q^2 + q + p^2 + pq + p = 191\\n   \\\\]\\n   Simplify:\\n   \\\\[\\n   p^2 + q^2 + 3pq + p + q = 191\\n   \\\\]\\n\\n3. **Introduce new variables:**\\n   Let \\\\( S = p + q \\\\) and \\\\( P = pq \\\\). The equation becomes:\\n   \\\\[\\n   S^2 + P + S = 191\\n   \\\\]\\n\\n4. **Express \\\\( r \\\\) in terms of \\\\( S \\\\):**\\n   Since \\\\( r = p + q + 1 = S + 1 \\\\), we need to find \\\\( S \\\\) such that \\\\( S^2 + P + S = 191 \\\\) and \\\\( P = pq \\\\) is the product of two primes \\\\( p \\\\) and \\\\( q \\\\) that sum to \\\\( S \\\\).\\n\\n5. **Determine possible values for \\\\( S \\\\):**\\n   Solve the inequality \\\\( S^2 + S < 191 \\\\):\\n   \\\\[\\n   S^2 + S - 191 < 0\\n   \\\\]\\n   Using the quadratic formula \\\\( S = \\\\frac{-1 \\\\pm \\\\sqrt{1 + 4 \\\\cdot 191}}{2} \\\\):\\n   \\\\[\\n   S = \\\\frac{-1 \\\\pm \\\\sqrt{765}}{2}\\n   \\\\]\\n   Since \\\\( \\\\sqrt{765} \\\\approx 27.66 \\\\), we have:\\n   \\\\[\\n   S \\\\approx \\\\frac{-1 + 27.66}{2} \\\\approx 13.33\\n   \\\\]\\n   Therefore, \\\\( S \\\\) must be an integer between 4 and 13.\\n\\n6. **Check possible values of \\\\( S \\\\):**\\n   - For \\\\( S = 12 \\\\):\\n     \\\\[\\n     P = 191 - 12^2 - 12 = 191 - 144 - 12 = 35\\n     \\\\]\\n     Check if \\\\( 35 \\\\) can be written as the product of two primes that sum to 12:\\n     \\\\[\\n     35 = 5 \\\\times 7 \\\\quad \\\\text{and} \\\\quad 5 + 7 = 12\\n     \\\\]\\n     This works. So \\\\( p = 5 \\\\) and \\\\( q = 7 \\\\).\\n\\n7. **Find \\\\( r \\\\):**\\n   \\\\[\\n   r = S + 1 = 12 + 1 = 13\\n   \\\\]\\n\\n8. **Verify the solution:**\\n   - Check \\\\( pq + qr + rp = 191 \\\\):\\n     \\\\[\\n     5 \\\\times 7 + 7 \\\\times 13 + 13 \\\\times 5 = 35 + 91 + 65 = 191\\n     \\\\]\\n   - Check \\\\( p + q = r - 1 \\\\):\\n     \\\\[\\n     5 + 7 = 12 \\\\quad \\\\text{and} \\\\quad 13 - 1 = 12\\n     \\\\]\\n\\nSince all conditions are satisfied, the final answer is:\\n\\\\[\\n\\\\boxed{25}\\n\\\\]\",\n          \"<think>\\nOkay, let's see. I need to solve this problem where x and y are positive integers satisfying 2(x + y) = gcd(x, y) + lcm(x, y). And I have to find the ratio of the lcm to the gcd of x and y. Hmm, okay, let's break this down.\\n\\nFirst, I remember that for any two positive integers, the product of the lcm and gcd of those numbers is equal to the product of the numbers themselves. So, lcm(x, y) * gcd(x, y) = x * y. That might come in handy here. Let me note that down: lcm(x,y)*gcd(x,y) = x*y.\\n\\nGiven the equation 2(x + y) = gcd(x, y) + lcm(x, y), maybe I can express everything in terms of gcd and the ratio of x and y. Since gcd and lcm are involved, it might be helpful to let d = gcd(x, y), and then express x and y as x = d*a and y = d*b, where a and b are coprime integers (their gcd is 1). That's a standard approach for problems involving gcd and lcm.\\n\\nSo let me set d = gcd(x, y). Then x = d*a, y = d*b, with gcd(a, b) = 1. Then, the lcm(x, y) would be d*a*b, because lcm(x, y) = x*y / gcd(x, y) = (d*a*d*b)/d = d*a*b. Right, so lcm(x,y) = d*a*b.\\n\\nSubstituting these into the original equation: 2(x + y) = gcd(x, y) + lcm(x, y).\\n\\nSubstituting x = d*a, y = d*b, gcd = d, lcm = d*a*b. Then:\\n\\n2(d*a + d*b) = d + d*a*b.\\n\\nFactor out d from the left side: 2d(a + b) = d(1 + a*b).\\n\\nSince d is a positive integer, we can divide both sides by d, yielding:\\n\\n2(a + b) = 1 + a*b.\\n\\nSo now the equation simplifies to 2(a + b) = a*b + 1, where a and b are coprime positive integers. Hmm, okay. Now we have a simpler equation to solve: a*b - 2a - 2b + 1 = 0. Let me rearrange that:\\n\\na*b - 2a - 2b + 1 = 0.\\n\\nHmm, maybe factor this equation? Let me see. Adding 4 to both sides might help in factoring. Let's try:\\n\\na*b - 2a - 2b + 1 + 4 - 4 = 0\\n\\nSo, a*b - 2a - 2b + 4 = 3.\\n\\nWait, not sure. Alternatively, perhaps rearrange the terms:\\n\\na*b - 2a - 2b = -1.\\n\\nThen, add 4 to both sides:\\n\\na*b - 2a - 2b + 4 = 3.\\n\\nNow, left side can be factored as (a - 2)(b - 2) = 3. Because expanding (a - 2)(b - 2) gives a*b - 2a - 2b + 4. Yes, that's right. So:\\n\\n(a - 2)(b - 2) = 3.\\n\\nSince a and b are positive integers and coprime, we need to find pairs (a, b) such that their product is 3 when each is reduced by 2. Also, since a and b are coprime, (a - 2) and (b - 2) must be divisors of 3, which is prime. The positive divisors of 3 are 1 and 3.\\n\\nSo possible pairs (since a and b are positive integers, a - 2 and b - 2 must be at least such that a and b are positive. Let's see:\\n\\nCase 1: (a - 2) = 1 and (b - 2) = 3. Then, a = 3, b = 5. Check if gcd(a, b) = 1. gcd(3, 5) = 1, which is good.\\n\\nCase 2: (a - 2) = 3 and (b - 2) = 1. Then, a = 5, b = 3. Similarly, gcd(5, 3) = 1. So this is also valid.\\n\\nBut also, since 3 is prime, the only positive divisors are 1 and 3. But since we're considering positive integers, we could also consider if one of them is negative? But since a and b are positive, a - 2 and b - 2 must be positive or zero? Wait, but 3 is positive, so the factors must both be positive. Because if one of (a - 2) or (b - 2) were negative, their product would be negative, but 3 is positive. So both (a - 2) and (b - 2) must be positive. Thus, only the two cases above.\\n\\nAlternatively, maybe (a - 2) and (b - 2) could be 3 and 1 in some order, which gives the two cases. So the possible (a, b) are (3, 5) and (5, 3). Since a and b are interchangeable (since x and y are symmetric in the problem), these two cases would yield the same results.\\n\\nSo now, let's see. For (a, b) = (3, 5), then x = d*3, y = d*5. Similarly, for (a, b) = (5, 3), x = d*5, y = d*3. But since the problem is symmetric in x and y, both cases are equivalent.\\n\\nNow, since we need to find the ratio lcm(x, y)/gcd(x, y), let's compute that.\\n\\nRecall that lcm(x, y)/gcd(x, y) = (d*a*b)/d = a*b. So it's simply a*b. Since in both cases, a and b are 3 and 5, the product is 15. Therefore, the ratio is 15.\\n\\nWait, that seems too straightforward. Let me check.\\n\\nIf the ratio is a*b, then yes. Because lcm(x, y) is d*a*b and gcd(x, y) is d, so their ratio is (d*a*b)/d = a*b. Since a and b are 3 and 5, 3*5=15. Therefore, the answer is 15. So the answer is 15.\\n\\nBut let me verify with an example. Let's take d=1. Then x=3, y=5. Then gcd(3,5)=1, lcm=15. Then 2(x + y) = 2*(8) = 16. The right side is 1 + 15=16. So that works. If d=1, then 2(3+5)=16=1 +15.\\n\\nWhat if d=2? Then x=6, y=10. gcd(6,10)=2, lcm=30. Then 2(6 + 10)=2*16=32. The right side is 2 +30=32. So that also works. Then the ratio lcm/gcd is 30/2=15. So regardless of d, the ratio is always 15. Wait, because if x = d*a, y = d*b, then lcm(x,y)/gcd(x,y) = (d*a*b)/d = a*b, which is 15 as in the first case. So regardless of d, the ratio is always 15. Therefore, the answer is 15.\\n\\nSo even if d is some other positive integer, the ratio remains a*b =15. Therefore, the required ratio is 15.\\n\\nTherefore, the answer is 15. So \\\\boxed{15}.\\n\\n**Final Answer**\\n\\\\boxed{15}\\n</think>Given \\\\( x \\\\) and \\\\( y \\\\) are positive integers such that \\\\( 2(x + y) = \\\\gcd(x, y) + \\\\text{lcm}(x, y) \\\\), we need to find \\\\( \\\\frac{\\\\text{lcm}(x, y)}{\\\\gcd(x, y)} \\\\).\\n\\nFirst, let \\\\( d = \\\\gcd(x, y) \\\\). Then, we can express \\\\( x \\\\) and \\\\( y \\\\) as \\\\( x = d \\\\cdot a \\\\) and \\\\( y = d \\\\cdot b \\\\), where \\\\( \\\\gcd(a, b) = 1 \\\\). The least common multiple (lcm) of \\\\( x \\\\) and \\\\( y \\\\) is given by \\\\( \\\\text{lcm}(x, y) = d \\\\cdot a \\\\cdot b \\\\).\\n\\nSubstituting these into the given equation:\\n\\\\[\\n2(d \\\\cdot a + d \\\\cdot b) = d + d \\\\cdot a \\\\cdot b\\n\\\\]\\nDividing both sides by \\\\( d \\\\):\\n\\\\[\\n2(a + b) = 1 + a \\\\cdot b\\n\\\\]\\nRearranging terms, we get:\\n\\\\[\\na \\\\cdot b - 2a - 2b + 1 = 0\\n\\\\]\\nAdding 4 to both sides to factorize:\\n\\\\[\\na \\\\cdot b - 2a - 2b + 4 = 3\\n\\\\]\\nThis can be factored as:\\n\\\\[\\n(a - 2)(b - 2) = 3\\n\\\\]\\nThe positive integer solutions for \\\\((a - 2)\\\\) and \\\\((b - 2)\\\\) are 1 and 3. Thus, the possible pairs \\\\((a, b)\\\\) are \\\\((3, 5)\\\\) and \\\\((5, 3)\\\\). Since \\\\( a \\\\) and \\\\( b \\\\) are coprime, both pairs are valid.\\n\\nThe ratio \\\\( \\\\frac{\\\\text{lcm}(x, y)}{\\\\gcd(x, y)} \\\\) is given by:\\n\\\\[\\n\\\\frac{\\\\text{lcm}(x, y)}{\\\\gcd(x, y)} = \\\\frac{d \\\\cdot a \\\\cdot b}{d} = a \\\\cdot b\\n\\\\]\\nFor both pairs \\\\((3, 5)\\\\) and \\\\((5, 3)\\\\), the product \\\\( a \\\\cdot b = 15 \\\\).\\n\\nThus, the final answer is:\\n\\\\[\\n\\\\boxed{15}\\n\\\\]\",\n          \"<think>\\nOkay, so I need to find the remainder when the product of all odd numbers from 1 to 2005 is divided by 1000. Hmm, let's think about how to approach this. \\n\\nFirst, the product is 1 \\u00d7 3 \\u00d7 5 \\u00d7 ... \\u00d7 2005. That's a lot of numbers! Since we're dealing with division by 1000, maybe modular arithmetic can help here. The remainder when divided by 1000 is equivalent to the product modulo 1000. But calculating such a huge product directly seems impossible. There has to be a smarter way.\\n\\nI remember that when dealing with factorials and remainders, factors of 2 and 5 can create trailing zeros. However, here we're only multiplying odd numbers, so there are no factors of 2. But there might still be factors of 5. Wait, 1000 is 8\\u00d7125, which is 2^3 \\u00d7 5^3. Since the product is all odd numbers, it won't have factors of 2, but it can have factors of 5. Therefore, the product will be divisible by 5^3, but since there are no 2s, the product modulo 1000 might not be zero. Hmm, maybe I need to compute the product modulo 1000, but adjusting for the factors of 5?\\n\\nAlternatively, maybe split the problem into modulo 8 and modulo 125, then use the Chinese Remainder Theorem (CRT) to combine the results. Since 1000 = 8 \\u00d7 125, and 8 and 125 are coprime, CRT says that if I can find the remainder modulo 8 and modulo 125, then I can combine them to find the remainder modulo 1000. That might be a good approach.\\n\\nLet me start with modulo 8. The product is 1\\u00d73\\u00d75\\u00d77\\u00d79\\u00d7...\\u00d72005. But modulo 8, odd numbers repeat every 8 numbers. Let's see, the residues modulo 8 of odd numbers are 1,3,5,7,1,3,5,7,... So the pattern repeats every 4 terms. Wait, no. Wait, the numbers go 1,3,5,7,9\\u22611,11\\u22613,13\\u22615,15\\u22617, etc. So every 8 numbers, the cycle of residues 1,3,5,7 repeats twice. Wait, actually, modulo 8, the odd residues cycle every 4 numbers. Let's confirm:\\n\\n1 mod 8 =1\\n\\n3 mod8=3\\n\\n5 mod8=5\\n\\n7 mod8=7\\n\\n9 mod8=1\\n\\n11 mod8=3\\n\\n13 mod8=5\\n\\n15 mod8=7\\n\\nYes, every 4 terms, the cycle repeats. So how many terms are in the product 1\\u00d73\\u00d75\\u00d7...\\u00d72005? Let's find the number of terms first. The nth odd number is 2n-1. So 2n-1=2005 => n=(2005+1)/2=2006/2=1003. So there are 1003 terms.\\n\\nSo 1003 terms, each group of 4 terms (mod8) is 1\\u00d73\\u00d75\\u00d77=105. Then 105 mod8= 105 - 13\\u00d78=105-104=1. So each group of 4 terms multiplies to 1 mod8. Then how many full groups of 4 are there in 1003 terms? Let's divide 1003 by 4. 1003 \\u00f74=250.75. So 250 full groups, each contributing 1 mod8, and then a remainder of 3 terms. \\n\\nSo the total product modulo8 is (1^250) \\u00d7 (last three terms). The last three terms would be the terms after the 250th group. The 250th group ends at term 250\\u00d74=1000. So the 1001st term is 2\\u00d71001 -1=2001. Wait, no: the first term is 1=2\\u00d71-1, second term 3=2\\u00d72-1, so term k is 2k-1. Therefore, term 1001 is 2\\u00d71001 -1=2002-1=2001. Then the 1002nd term is 2003, 1003rd term is 2005. So the last three terms are 2001, 2003, 2005. Let's compute each mod8:\\n\\n2001 \\u00f78: 8\\u00d7250=2000, so 2001 mod8=1\\n\\n2003 mod8=3\\n\\n2005 mod8=5\\n\\nSo the last three terms modulo8 are 1\\u00d73\\u00d75=15 mod8=7.\\n\\nTherefore, total product mod8 is (1^250) \\u00d77=1\\u00d77=7 mod8.\\n\\nSo the remainder modulo8 is7.\\n\\nNow, we need to compute the product modulo125. This seems more complicated. Let's think.\\n\\nThe product is the product of all odd numbers from1 to2005. Wait, 2005=5\\u00d7401. So we can write the product as (1\\u00d73\\u00d75\\u00d77\\u00d7...\\u00d72005). Let's note that there are a lot of factors of 5 in this product, which would make the product divisible by 5 multiple times. However, modulo125 is 5^3, so if the product has at least three factors of 5, then modulo125 would be 0. Wait, but maybe even if it's divisible by 5^3, but we need to compute the actual remainder. Wait, but perhaps the product is divisible by 5^3, but when divided by 5^3, the remaining product modulo8 or something else. Wait, maybe not. Let me check how many factors of5 are in the product.\\n\\nThe number of factors of5 in the product:\\n\\nEach multiple of5 contributes at least one factor of5. Since we're dealing with odd numbers, the multiples of5 that are odd. So numbers divisible by5 but not by2. So numbers like5,15,25,...,2005. Let's count how many multiples of5 are in the product. The first term is5, which is5\\u00d71, then15=5\\u00d73,..., up to2005=5\\u00d7401. So the multiples of5 are5\\u00d7(1,3,5,...,401). Wait, 5\\u00d7k, where k is odd from1 to401. Because 5\\u00d7401=2005. So how many terms are there?\\n\\nThe number of terms k from1 to401 where k is odd. Since401 is odd, the number is (401 +1)/2=201. So there are201 multiples of5 in the product. Each contributes at least one factor of5. Additionally, multiples of25 contribute an extra factor of5. Similarly, multiples of125, 625, etc., contribute more factors.\\n\\nSo let's compute the total number of factors of5 in the product.\\n\\nNumber of multiples of5:201 (as above)\\n\\nNumber of multiples of25: These are numbers in the product divisible by25. Since the product includes numbers of the form5\\u00d7(odd numbers). So multiples of25 are numbers divisible by25, which are 25,75,125,...,2000. But 2005 is not divisible by25. Wait, wait, in the original product (all odd numbers up to2005), the multiples of25 must be odd multiples. So 25\\u00d71,25\\u00d73,..., up to the largest odd multiple less than or equal to2005.\\n\\n25\\u00d7k \\u22642005, where k is odd. Let's compute k_max:\\n\\n25k \\u22642005 => k \\u22642005/25=80.2. So the largest integer k is80, but since k has to be odd, the largest odd k is79. So 25\\u00d779=1975. Then 25\\u00d781=2025, which is over. So the multiples of25 in the product are25\\u00d71,25\\u00d73,...,25\\u00d779. Number of terms: (79-1)/2 +1=39 +1=40. Wait, from1 to79 odd numbers: number is (79+1)/2=40. So there are40 multiples of25.\\n\\nSimilarly, multiples of125:125\\u00d71,125\\u00d73,..., up to125\\u00d7k\\u22642005. 125\\u00d7k \\u22642005 =>k\\u226416.04. So k_max=15 (odd). So 125\\u00d715=1875. So multiples are125\\u00d71,125\\u00d73,...,125\\u00d715. Number of terms: (15-1)/2 +1=7+1=8.\\n\\nMultiples of625:625\\u00d71=625, next is625\\u00d73=1875, next is625\\u00d75=3125>2005. So only two multiples:625 and1875. But1875 is already counted as a multiple of125. So factors of625 contribute an extra factor of5 each. So number of multiples of625 is 2. 625 and1875.\\n\\nMultiples of3125:3125>2005, so none.\\n\\nSo total number of factors of5:\\n\\nFrom multiples of5:201\\n\\nFrom multiples of25:40 (each contributes an extra)\\n\\nFrom multiples of125:8 (each contributes another extra)\\n\\nFrom multiples of625:2 (each contributes another extra)\\n\\nTotal:201 +40 +8 +2=251.\\n\\nSo total factors of5 in the product:251.\\n\\nSimilarly, factors of2: since all numbers are odd, there are none. So the product is divisible by5^251 but not by2. So when dividing by5^3, since 251\\u22653, the product is divisible by5^3. Therefore, the product modulo125 is0? Wait, no, wait. Wait, 125 is5^3. If the product has at least three factors of5, then when divided by5^3, the quotient is an integer, but the remainder when divided by125 is0. So if the product is divisible by125, then the remainder is0. But the question is, when we divide the product by1000, which is8\\u00d7125, the remainder is to be found. However, we already considered that modulo8 is7, and modulo125 is... Hmm, but if modulo125 is0, then using CRT, we can say the remainder is a number congruent to7 mod8 and0 mod125. So we need to solve for x\\u22610 mod125 andx\\u22617 mod8.\\n\\nBut wait, let me confirm if modulo125 is indeed0. Let's check: since the product has at least three factors of5, then yes, the product is divisible by5^3, so product \\u22610 mod125. Therefore, modulo125 is0. Therefore, we have:\\n\\nx \\u22617 mod8\\n\\nx \\u22610 mod125\\n\\nWe need to find x such that x \\u22610 mod125 andx \\u22617 mod8. Let\\u2019s solve this system.\\n\\nLet x=125k. Then 125k\\u22617 mod8. Since125 mod8=5, so 5k\\u22617 mod8.\\n\\nWe solve 5k\\u22617 mod8.\\n\\nMultiply both sides by inverse of5 mod8. The inverse of5 mod8 is5, since5\\u00d75=25\\u22611 mod8. So multiply both sides by5:\\n\\nk\\u22617\\u00d75 mod8 =>k\\u226135 mod8 =>35\\u00f78=4*8=32, 35-32=3. So k\\u22613 mod8.\\n\\nTherefore, k=8m +3 for some integer m. Therefore, x=125(8m +3)=1000m +375. Therefore, the smallest non-negative solution is375. Therefore, the remainder is375 when divided by1000. So the answer is375.\\n\\nWait, but before accepting that, let me verify my steps again because this is tricky.\\n\\nFirst, confirming that the product is divisible by5^3: yes, since there are 251 factors of5, so 5^251 divides the product. Therefore, the product is divisible by5^3, so product\\u22610 mod125. That's correct.\\n\\nThen, solving x\\u22617 mod8 and x\\u22610 mod125. So x=125k. Then 125k\\u22615k mod8. So 5k\\u22617 mod8. Multiply both sides by inverse of5 mod8, which is5, since5\\u00d75=25\\u22611 mod8. So k\\u226135 mod8\\u22613 mod8. Therefore, k=8m +3, so x=125\\u00d73 +1000m=375 +1000m. So the minimal positive solution is375. So remainder is375 when divided by1000. That seems correct.\\n\\nBut wait, to make sure, let me check with an example. Let's take x=375. 375 \\u00f78=46*8=368, remainder7. So 375 mod8=7. 375 mod125=0. So yes, 375 satisfies both conditions. Therefore, the remainder is375. Therefore, the answer is\\\\boxed{375}.\\n\\nWait, but I need to make sure that my calculation for modulo8 and modulo125 are correct. Let me double-check the modulo8 calculation.\\n\\nEarlier, I considered that the product modulo8 is7. Let me recast that.\\n\\nThe product is1\\u00d73\\u00d75\\u00d77\\u00d7\\u2026\\u00d72005. When taking modulo8, each cycle of4 terms (1,3,5,7) multiplies to1\\u00d73\\u00d75\\u00d77=105\\u22611 mod8. Since 105/8=13\\u00d78=104, 105-104=1. Then, since there are1003 terms, how many cycles of4 are there? 1003 divided by4 is250 cycles with a remainder of3 terms. Then the product is (1)^250 \\u00d7 last3 terms.\\n\\nThe last3 terms are2001,2003,2005. 2001 mod8=1 (2001-8\\u00d7250=2001-2000=1), 2003=2001+2\\u21921+2=3 mod8, 2005=2001+4\\u21921+4=5 mod8. So last3 terms are1\\u00d73\\u00d75=15\\u22617 mod8. Therefore, total product\\u22611^250 \\u00d77\\u22617 mod8. Correct.\\n\\nTherefore, the logic holds. Therefore, the remainder is375. So I think that's the correct answer.\\n\\n**Final Answer**\\n\\\\boxed{375}\\n</think>To find the remainder when the product \\\\(1 \\\\times 3 \\\\times 5 \\\\times \\\\cdots \\\\times 2005\\\\) is divided by 1000, we use modular arithmetic and the Chinese Remainder Theorem (CRT).\\n\\n### Step 1: Calculate the product modulo 8\\n\\nThe sequence of odd numbers modulo 8 repeats every 4 terms: \\\\(1, 3, 5, 7\\\\). The product of each cycle is:\\n\\\\[\\n1 \\\\times 3 \\\\times 5 \\\\times 7 = 105 \\\\equiv 1 \\\\mod 8\\n\\\\]\\n\\nThere are 1003 terms in the product. Dividing 1003 by 4 gives 250 full cycles and a remainder of 3 terms. The remaining terms are 2001, 2003, and 2005. We calculate these modulo 8:\\n\\\\[\\n2001 \\\\equiv 1 \\\\mod 8, \\\\quad 2003 \\\\equiv 3 \\\\mod 8, \\\\quad 2005 \\\\equiv 5 \\\\mod 8\\n\\\\]\\nThe product of these remaining terms is:\\n\\\\[\\n1 \\\\times 3 \\\\times 5 = 15 \\\\equiv 7 \\\\mod 8\\n\\\\]\\n\\nThus, the product modulo 8 is:\\n\\\\[\\n1^{250} \\\\times 7 \\\\equiv 7 \\\\mod 8\\n\\\\]\\n\\n### Step 2: Calculate the product modulo 125\\n\\nWe need to determine the number of factors of 5 in the product. The sequence of odd numbers includes multiples of 5, 25, 125, and 625.\\n\\n- Multiples of 5: \\\\(5, 15, 25, \\\\ldots, 2005\\\\)\\n  - These are of the form \\\\(5 \\\\times (2k+1)\\\\) for \\\\(k = 0, 1, 2, \\\\ldots, 200\\\\)\\n  - Number of such terms: \\\\(\\\\frac{2005}{5} = 401\\\\), and half of these are odd, so \\\\(201\\\\) multiples of 5.\\n\\n- Multiples of 25: \\\\(25, 75, 125, \\\\ldots, 1975\\\\)\\n  - These are of the form \\\\(25 \\\\times (2k+1)\\\\) for \\\\(k = 0, 1, 2, \\\\ldots, 39\\\\)\\n  - Number of such terms: \\\\(\\\\frac{1975}{25} = 79\\\\), and half of these are odd, so \\\\(40\\\\) multiples of 25.\\n\\n- Multiples of 125: \\\\(125, 375, 625, 875, 1125, 1375, 1625, 1875\\\\)\\n  - These are of the form \\\\(125 \\\\times (2k+1)\\\\) for \\\\(k = 0, 1, 2, \\\\ldots, 15\\\\)\\n  - Number of such terms: \\\\(\\\\frac{1875}{125} = 15\\\\), and half of these are odd, so \\\\(8\\\\) multiples of 125.\\n\\n- Multiples of 625: \\\\(625, 1875\\\\)\\n  - These are of the form \\\\(625 \\\\times (2k+1)\\\\) for \\\\(k = 0, 1\\\\)\\n  - Number of such terms: \\\\(\\\\frac{1875}{625} = 3\\\\), and half of these are odd, so \\\\(2\\\\) multiples of 625.\\n\\nTotal factors of 5:\\n\\\\[\\n201 + 40 + 8 + 2 = 251\\n\\\\]\\n\\nSince \\\\(251 \\\\geq 3\\\\), the product is divisible by \\\\(5^3 = 125\\\\). Therefore, the product modulo 125 is:\\n\\\\[\\n0 \\\\mod 125\\n\\\\]\\n\\n### Step 3: Combine results using the Chinese Remainder Theorem\\n\\nWe have:\\n\\\\[\\nx \\\\equiv 7 \\\\mod 8\\n\\\\]\\n\\\\[\\nx \\\\equiv 0 \\\\mod 125\\n\\\\]\\n\\nLet \\\\(x = 125k\\\\). Then:\\n\\\\[\\n125k \\\\equiv 7 \\\\mod 8\\n\\\\]\\nSince \\\\(125 \\\\equiv 5 \\\\mod 8\\\\), we have:\\n\\\\[\\n5k \\\\equiv 7 \\\\mod 8\\n\\\\]\\n\\nThe multiplicative inverse of 5 modulo 8 is 5, so:\\n\\\\[\\nk \\\\equiv 7 \\\\times 5 \\\\equiv 35 \\\\equiv 3 \\\\mod 8\\n\\\\]\\n\\nThus, \\\\(k = 8m + 3\\\\) for some integer \\\\(m\\\\). Therefore:\\n\\\\[\\nx = 125(8m + 3) = 1000m + 375\\n\\\\]\\n\\nThe smallest non-negative solution is:\\n\\\\[\\nx = 375\\n\\\\]\\n\\nTherefore, the remainder when the product is divided by 1000 is:\\n\\\\[\\n\\\\boxed{375}\\n\\\\]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "dataset = load_dataset(\"unsloth/OpenMathReasoning-mini\", split = \"cot\")\n",
        "dataset = dataset.to_pandas()[\n",
        "    [\"expected_answer\", \"problem\", \"generated_solution\"]\n",
        "]\n",
        "\n",
        "# Try converting to number - if not, replace with NaN\n",
        "is_number = pd.to_numeric(pd.Series(dataset[\"expected_answer\"]), errors = \"coerce\").notnull()\n",
        "# Select only numbers\n",
        "dataset = dataset.iloc[np.where(is_number)[0]]\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVRFqoSdIEVK"
      },
      "source": [
        "We have to format the dataset to follow our GRPO style formatting:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Z9ydcV_Abfi6"
      },
      "outputs": [],
      "source": [
        "def format_dataset(x):\n",
        "    expected_answer = x[\"expected_answer\"]\n",
        "    problem = x[\"problem\"]\n",
        "\n",
        "    # Remove generated <think> and </think>\n",
        "    thoughts = x[\"generated_solution\"]\n",
        "    thoughts = thoughts.replace(\"<think>\", \"\").replace(\"</think>\", \"\")\n",
        "\n",
        "    # Strip newlines on left and right\n",
        "    thoughts = thoughts.strip()\n",
        "    # Add our custom formatting\n",
        "    final_prompt = \\\n",
        "        reasoning_start + thoughts + reasoning_end + \\\n",
        "        solution_start + expected_answer + solution_end\n",
        "    return [\n",
        "        {\"role\" : \"system\",    \"content\" : system_prompt},\n",
        "        {\"role\" : \"user\",      \"content\" : problem},\n",
        "        {\"role\" : \"assistant\", \"content\" : final_prompt},\n",
        "    ]\n",
        "\n",
        "dataset[\"Messages\"] = dataset.apply(format_dataset, axis = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5NI47rOIRP2"
      },
      "source": [
        "Check to see if it worked:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "LTdXBKcslhRH",
        "outputId": "7508f91b-4d3e-425e-b035-79c3a1e0ec02"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"You are given a problem.\\nThink about the problem and provide your working out.\\nPlace it between <start_working_out> and <end_working_out>.\\nThen, provide your solution between <SOLUTION></SOLUTION><|eot_id|>Given $\\\\sqrt{x^2+165}-\\\\sqrt{x^2-52}=7$ and $x$ is positive, find all possible values of $x$.<start_working_out>Okay, let's see. I need to solve the equation ‚àö(x¬≤ + 165) - ‚àö(x¬≤ - 52) = 7, and find all positive values of x. Hmm, radicals can be tricky, but maybe if I can eliminate the square roots by squaring both sides. Let me try that.\\n\\nFirst, let me write down the equation again to make sure I have it right:\\n\\n‚àö(x¬≤ + 165) - ‚àö(x¬≤ - 52) = 7.\\n\\nOkay, so the idea is to isolate one of the radicals and then square both sides. Let me try moving the second radical to the other side:\\n\\n‚àö(x¬≤ + 165) = 7 + ‚àö(x¬≤ - 52).\\n\\nNow, if I square both sides, maybe I can get rid of the square roots. Let's do that:\\n\\n(‚àö(x¬≤ + 165))¬≤ = (7 + ‚àö(x¬≤ - 52))¬≤.\\n\\nSimplifying the left side:\\n\\nx¬≤ + 165 = 49 + 14‚àö(x¬≤ - 52) + (‚àö(x¬≤ - 52))¬≤.\\n\\nThe right side is expanded using the formula (a + b)¬≤ = a¬≤ + 2ab + b¬≤. So the right side becomes 7¬≤ + 2*7*‚àö(x¬≤ - 52) + (‚àö(x¬≤ - 52))¬≤, which is 49 + 14‚àö(x¬≤ - 52) + (x¬≤ - 52).\\n\\nSo putting it all together:\\n\\nx¬≤ + 165 = 49 + 14‚àö(x¬≤ - 52) + x¬≤ - 52.\\n\\nHmm, let's simplify the right side. The x¬≤ terms will cancel out, right? Let's subtract x¬≤ from both sides:\\n\\n165 = 49 + 14‚àö(x¬≤ - 52) - 52.\\n\\nSimplify the constants on the right:\\n\\n49 - 52 is -3, so:\\n\\n165 = -3 + 14‚àö(x¬≤ - 52).\\n\\nNow, add 3 to both sides to isolate the radical term:\\n\\n165 + 3 = 14‚àö(x¬≤ - 52).\\n\\nSo 168 = 14‚àö(x¬≤ - 52).\\n\\nDivide both sides by 14:\\n\\n168 / 14 = ‚àö(x¬≤ - 52).\\n\\n12 = ‚àö(x¬≤ - 52).\\n\\nNow, square both sides again to eliminate the square root:\\n\\n12¬≤ = x¬≤ - 52.\\n\\n144 = x¬≤ - 52.\\n\\nAdd 52 to both sides:\\n\\n144 + 52 = x¬≤.\\n\\n196 = x¬≤.\\n\\nSo x = ‚àö196 = 14.\\n\\nBut wait, since the problem states that x is positive, we only take the positive root. So x = 14.\\n\\nBut hold on, when dealing with squaring equations, sometimes extraneous solutions can come up. I should check if this solution actually satisfies the original equation.\\n\\nLet's plug x = 14 back into the original equation:\\n\\n‚àö(14¬≤ + 165) - ‚àö(14¬≤ - 52) = ?\\n\\nCalculate each term:\\n\\n14¬≤ is 196.\\n\\nSo first radical: ‚àö(196 + 165) = ‚àö361 = 19.\\n\\nSecond radical: ‚àö(196 - 52) = ‚àö144 = 12.\\n\\nSo 19 - 12 = 7, which is exactly the right-hand side. So yes, it checks out.\\n\\nTherefore, the only solution is x = 14. Since the problem says x is positive, we don't have to consider negative roots. So I think that's the answer.\\nTo solve the equation \\\\(\\\\sqrt{x^2 + 165} - \\\\sqrt{x^2 - 52} = 7\\\\) for positive \\\\(x\\\\), we proceed as follows:\\n\\n1. Start with the given equation:\\n   \\\\[\\n   \\\\sqrt{x^2 + 165} - \\\\sqrt{x^2 - 52} = 7\\n   \\\\]\\n\\n2. Isolate one of the square roots by moving \\\\(\\\\sqrt{x^2 - 52}\\\\) to the right side:\\n   \\\\[\\n   \\\\sqrt{x^2 + 165} = 7 + \\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n3. Square both sides to eliminate the square root on the left:\\n   \\\\[\\n   (\\\\sqrt{x^2 + 165})^2 = (7 + \\\\sqrt{x^2 - 52})^2\\n   \\\\]\\n   Simplifying both sides, we get:\\n   \\\\[\\n   x^2 + 165 = 49 + 14\\\\sqrt{x^2 - 52} + (x^2 - 52)\\n   \\\\]\\n\\n4. Combine like terms on the right side:\\n   \\\\[\\n   x^2 + 165 = x^2 - 52 + 49 + 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n   Simplifying further:\\n   \\\\[\\n   x^2 + 165 = x^2 - 3 + 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n5. Subtract \\\\(x^2\\\\) from both sides:\\n   \\\\[\\n   165 = -3 + 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n6. Add 3 to both sides to isolate the term with the square root:\\n   \\\\[\\n   168 = 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n7. Divide both sides by 14:\\n   \\\\[\\n   12 = \\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n8. Square both sides again to eliminate the square root:\\n   \\\\[\\n   12^2 = x^2 - 52\\n   \\\\]\\n   Simplifying:\\n   \\\\[\\n   144 = x^2 - 52\\n   \\\\]\\n\\n9. Add 52 to both sides to solve for \\\\(x^2\\\\):\\n   \\\\[\\n   196 = x^2\\n   \\\\]\\n\\n10. Take the positive square root (since \\\\(x\\\\) is positive):\\n    \\\\[\\n    x = \\\\sqrt{196} = 14\\n    \\\\]\\n\\n11. Verify the solution by substituting \\\\(x = 14\\\\) back into the original equation:\\n    \\\\[\\n    \\\\sqrt{14^2 + 165} - \\\\sqrt{14^2 - 52} = \\\\sqrt{196 + 165} - \\\\sqrt{196 - 52} = \\\\sqrt{361} - \\\\sqrt{144} = 19 - 12 = 7\\n    \\\\]\\n    The solution checks out.\\n\\nThus, the only positive solution is:\\n\\\\[\\n\\\\boxed{14}\\n\\\\]<end_working_out><SOLUTION>14</SOLUTION><|eot_id|>\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "tokenizer.apply_chat_template(dataset[\"Messages\"][0], tokenize = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHV9BXYiIYaq"
      },
      "source": [
        "Let's truncate the pre fine-tuning dataset to `max_seq_length/2` since we don't want too long reasoning traces.\n",
        "\n",
        "Note this might take 2 minutes!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBHFlRbae9_s",
        "outputId": "25afb07c-5333-4229-a664-a3bbd8b946a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(78, 5)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[\"N\"] = dataset[\"Messages\"].apply(lambda x: len(tokenizer.apply_chat_template(x)))\n",
        "\n",
        "dataset = dataset.loc[dataset[\"N\"] <= max_seq_length/2].copy()\n",
        "dataset.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6NkUCAGIj8N"
      },
      "source": [
        "We then tokenize the messages and convert it to a Hugging Face compatible dataset format:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rgdtiV_f5hx",
        "outputId": "69321940-773b-4865-96c8-fbbf44baccec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['expected_answer', 'problem', 'generated_solution', 'Messages', 'N', 'text', '__index_level_0__'],\n",
              "    num_rows: 78\n",
              "})"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "dataset[\"text\"] = tokenizer.apply_chat_template(dataset[\"Messages\"].values.tolist(), tokenize = False)\n",
        "dataset = Dataset.from_pandas(dataset)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAQJjQrYKzOk"
      },
      "source": [
        "Let's now pre fine-tune the model so it follows our custom GRPO formatting!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105,
          "referenced_widgets": [
            "8fd2ee6228fe44c9bf5d31b1058da778",
            "7cacb838c2f04b098fe19f26bfec54d1",
            "6028501c29af4da3815fb2a52467a432",
            "a709c0effc894e61bc59c10f9bfc8d79",
            "4522ea35d20041888cd824bf90253d22",
            "735479bf12ea4211befdec6a05f17c3e",
            "0304e9d55f9c48aab8719294905cfdf0",
            "83b334b61b1e42d8882581991caa8559",
            "b1648d3ac6594e4d9d25e07d5a91a850",
            "942bf492503a4a729a76b549b3a7278b",
            "138f549b6f7744b6beed345e06ebc81a"
          ]
        },
        "id": "woYi0SSygpqp",
        "outputId": "a8ec1067-51b0-47b5-bfac-3267c5aea897"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:trl.trainer.sft_trainer:You are using a per_device_train_batch_size of 1 with padding-free training. Using a batch size of 1 anihilate the benefits of padding-free training. Please consider increasing the batch size to at least 2.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8fd2ee6228fe44c9bf5d31b1058da778",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/78 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü¶• Unsloth: Padding-free auto-enabled, enabling faster training.\n"
          ]
        }
      ],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    args = SFTConfig(\n",
        "        dataset_text_field = \"text\",\n",
        "        per_device_train_batch_size = 1,\n",
        "        gradient_accumulation_steps = 1, # Use GA to mimic batch size!\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 2, # Set this for 1 full training run.\n",
        "        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
        "        logging_steps = 5,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.001,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        report_to = \"none\", # Use TrackIO/WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "l4-2v_bLhZuE",
        "outputId": "434a36a8-911b-4871-f1dc-cf5f39e2a721"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 78 | Num Epochs = 2 | Total steps = 156\n",
            "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 1\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 1 x 1) = 1\n",
            " \"-____-\"     Trainable parameters = 22,544,384 of 1,258,358,784 (1.79% trained)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='156' max='156' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [156/156 01:10, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.238400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.955400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.777900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.817400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.728000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.741300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.747900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.706900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.644800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.726300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.622400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.585500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.706800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.771000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.629300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.641500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.425000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.405400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.548400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.445800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>0.379300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.375000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>0.478100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.354200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.379900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.416600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>0.411500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.369700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>0.410000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.315100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>0.405700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=156, training_loss=0.5855241318543752, metrics={'train_runtime': 73.3967, 'train_samples_per_second': 2.125, 'train_steps_per_second': 2.125, 'total_flos': 836105206063104.0, 'train_loss': 0.5855241318543752, 'epoch': 2.0})"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRMBNUBgLC8T"
      },
      "source": [
        "Let's check if the model has learnt to follow the custom format:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HJxrS76h3Ds",
        "outputId": "774b3b68-d8c6-4757-bde0-1c6ac5761149"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|>You are given a problem.\n",
            "Think about the problem and provide your working out.\n",
            "Place it between <start_working_out> and <end_working_out>.\n",
            "Then, provide your solution between <SOLUTION></SOLUTION><|eot_id|>Jenifer has 82 cents in pennies and nickels. Her younger brother mistook all her nickels for dimes and counted the total as $1.47. How many pennies does Jenifer have?<start_working_out>Okay, let's see. So the problem is about Jenifer who has 82 cents in pennies and nickels. Her brother thought all the nickels were dimes and counted the total as $1.47. We need to find out how many pennies Jenifer has. Hmm, let's break this down step by step.\n",
            "\n",
            "First, let me remember that a nickel is worth 5 cents and a dime is worth 10 cents. So if her brother thought nickels were dimes, then he counted the total as if all nickels were worth 10 cents each. Let me denote the number of nickels as N. Then the total amount she had would be 5N cents. Since her brother counted it as $1.47, which is 147 cents, that means the total in cents is 147. So 5N = 147. Let me solve for N.\n",
            "\n",
            "5N = 147\n",
            "N = 147 / 5\n",
            "N = 29.4\n",
            "\n",
            "Wait, so N is 29.4 nickels. But since you can't have a fraction of a coin, that means she has 29 nickels. Now, each nickel is 5 cents, so 29 nickels would be 29 * 5 = 145 cents. That's less than 147, so that's correct. So she has 29 nickels.\n",
            "\n",
            "Now, each nickel is worth 5 cents, so 29 nickels total 145 cents. That's 29 * 5 = 145 cents. So she has 145 cents in nickels. Since she started with 82 cents, that means she has 145 - 82 = 63 cents left in pennies. Let me check that again.\n",
            "\n",
            "Total nickels: 29 * 5 = 145 cents\n",
            "Total pennies: 82 cents\n",
            "63 cents left over, so 145 - 82 = 63. Yep, that makes sense. So she has 63 pennies.\n",
            "\n",
            "Wait, but let me make sure I didn't mix up the problem. The problem says her brother thought all nickels were dimes. So maybe she had 29 nickels, and each nickel is a dime. Then 29 dimes would be 29 * 10 = 290 cents. That's more than 147 cents. So that's not possible. So she must have had 29 nickels. Then the total is 29 * 5 = 145 cents. Subtracting 82 cents gives 63 cents left over. So yes, that works. So she has 63 pennies.\n",
            "\n",
            "But let me check again. If she had 29 nickels, then each nickel is 5 cents. So 29 nickels total 145 cents. Subtracting 82 cents gives 63 cents left over. So that's correct. So the answer should be 63 pennies. I think that's right. So the final answer is 63.\n",
            "To determine how many pennies Jenifer has, we start by defining the variables:\n",
            "- \\( N \\) is the number of nickels.\n",
            "- \\( 5N \\) is the total amount of money in cents, where each nickel is worth 5 cents.\n",
            "\n",
            "Given that her brother mistook all nickels for dimes and counted the total as $1.47 (147 cents), we can set up the following equation:\n",
            "\\[\n",
            "5N = 147\n",
            "\\]\n",
            "To find \\( N \\), we solve for \\( N \\):\n",
            "\\[\n",
            "N = \\frac{147}{5} = 29.4\n",
            "\\]\n",
            "Since \\( N \\) must be a whole number (you can't have a fraction of a coin), we conclude that Jenifer has 29 nickels.\n",
            "\n",
            "Next, we calculate the total amount of money in cents:\n",
            "\\[\n",
            "29 \\times 5 = 145 \\text{ cents}\n",
            "\\]\n",
            "Jenifer initially had 82 cents. The total amount of money she has is:\n",
            "\\[\n",
            "145 \\text{ cents} - 82 \\text{ cents} = 63 \\text{ cents}\n",
            "\\]\n",
            "To find the number of pennies, we subtract 63 cents from the total amount:\n",
            "\\[\n",
            "63 \\text{ cents} - 63 \\text{ cents} = 0 \\text{ pennies}\n",
            "\\]\n",
            "Thus, Jenifer has 0 pennies.\n",
            "\n",
            "The final answer is: $\\boxed{0}$<end_working_out><SOLUTION>0</SOLUTION><|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "text = tokenizer.apply_chat_template(\n",
        "    dataset[0][\"Messages\"][:2],\n",
        "    tokenize = False,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        ")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "_ = model.generate(\n",
        "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
        "    temperature = 0,\n",
        "    max_new_tokens = 1024,\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = False),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtZ3qGOALF95"
      },
      "source": [
        "Yes it did follow the formatting! Great! Let's remove some items before the GRPO step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWSZ0DET7bob",
        "outputId": "c1af7c7e-3d18-4021-be81-844d51a40b7d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "146"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "del dataset\n",
        "torch.cuda.empty_cache()\n",
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KGgPgk_5S8r"
      },
      "source": [
        "### Data Prep\n",
        "<a name=\"Data\"></a>\n",
        "\n",
        "We're using Hugging Face's [Open R1 Math dataset](https://huggingface.co/datasets/open-r1/DAPO-Math-17k-Processed). You can also utilize OpenAI's famous [GSM8K dataset](https://huggingface.co/datasets/openai/gsm8k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7-eUrQn-OzE",
        "outputId": "68eef0de-7f07-4a98-fd49-b48f03287010"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['prompt', 'solution', 'data_source', 'source_prompt', 'ability', 'reward_model', 'extra_info'],\n",
              "    num_rows: 14116\n",
              "})"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"open-r1/DAPO-Math-17k-Processed\", \"en\", split = \"train\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b00gUsS-ROW"
      },
      "source": [
        "Let's look at the first row:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "siopxjG8-ReF",
        "outputId": "4cd0166b-53c6-4ee6-d48f-bdf05f7d001e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'In triangle $ABC$, $\\\\sin \\\\angle A = \\\\frac{4}{5}$ and $\\\\angle A < 90^\\\\circ$. Let $D$ be a point outside triangle $ABC$ such that $\\\\angle BAD = \\\\angle DAC$ and $\\\\angle BDC = 90^\\\\circ$. Suppose that $AD = 1$ and that $\\\\frac{BD}{CD} = \\\\frac{3}{2}$. If $AB + AC$ can be expressed in the form $\\\\frac{a\\\\sqrt{b}}{c}$ where $a, b, c$ are pairwise relatively prime integers, find $a + b + c$.'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[0][\"prompt\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KGupRQqD-Wcf",
        "outputId": "d1e95d48-15e5-44b8-9596-40c7f8268d1a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'34'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[0][\"solution\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmnXj6hn-Ydi"
      },
      "source": [
        "In GSM8K, ee notice all answers like about have a ####, so we extract it. But for the Open R1 dataset, we can skip the below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8JJGXKdJ-Zl_",
        "outputId": "79a3adab-5654-499e-b983-1fd4bad24000"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'34'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def extract_hash_answer(text):\n",
        "    # if \"####\" not in text: return None\n",
        "    # return text.split(\"####\")[1].strip()\n",
        "    return text\n",
        "extract_hash_answer(dataset[0][\"solution\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K30CygaU-dir"
      },
      "source": [
        "Let's map the dataset! and see the first row:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyEVI972-d3n",
        "outputId": "1f359a9a-c4e7-4768-d9e4-24d85f285481"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'prompt': [{'content': 'You are given a problem.\\nThink about the problem and provide your working out.\\nPlace it between <start_working_out> and <end_working_out>.\\nThen, provide your solution between <SOLUTION></SOLUTION>',\n",
              "   'role': 'system'},\n",
              "  {'content': 'In triangle $ABC$, $\\\\sin \\\\angle A = \\\\frac{4}{5}$ and $\\\\angle A < 90^\\\\circ$. Let $D$ be a point outside triangle $ABC$ such that $\\\\angle BAD = \\\\angle DAC$ and $\\\\angle BDC = 90^\\\\circ$. Suppose that $AD = 1$ and that $\\\\frac{BD}{CD} = \\\\frac{3}{2}$. If $AB + AC$ can be expressed in the form $\\\\frac{a\\\\sqrt{b}}{c}$ where $a, b, c$ are pairwise relatively prime integers, find $a + b + c$.',\n",
              "   'role': 'user'}],\n",
              " 'solution': '34',\n",
              " 'data_source': 'math_dapo',\n",
              " 'source_prompt': [{'content': 'Solve the following math problem step by step. The last line of your response should be of the form Answer: $Answer (without quotes) where $Answer is the answer to the problem.\\n\\nIn triangle $ABC$, $\\\\sin \\\\angle A = \\\\frac{4}{5}$ and $\\\\angle A < 90^\\\\circ$. Let $D$ be a point outside triangle $ABC$ such that $\\\\angle BAD = \\\\angle DAC$ and $\\\\angle BDC = 90^\\\\circ$. Suppose that $AD = 1$ and that $\\\\frac{BD}{CD} = \\\\frac{3}{2}$. If $AB + AC$ can be expressed in the form $\\\\frac{a\\\\sqrt{b}}{c}$ where $a, b, c$ are pairwise relatively prime integers, find $a + b + c$.\\n\\nRemember to put your answer on its own line after \"Answer:\".',\n",
              "   'role': 'user'}],\n",
              " 'ability': 'MATH',\n",
              " 'reward_model': {'ground_truth': '34', 'style': 'rule-lighteval/MATH_v2'},\n",
              " 'extra_info': {'index': '9a9b6eb4-a1cb-49d1-8c1e-62eaf2f74079'},\n",
              " 'answer': '34'}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = dataset.map(lambda x: {\n",
        "    \"prompt\" : [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\",   \"content\": x[\"prompt\"]},\n",
        "    ],\n",
        "    \"answer\": extract_hash_answer(x[\"solution\"]),\n",
        "})\n",
        "dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9m8eR9T-gMh"
      },
      "source": [
        "We create a regex format to match the reasoning sections and answers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQwjTjNz-gY_",
        "outputId": "dbfbbebb-7b66-4977-b9fe-1ff7e86b3de4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "re.compile(r'<end_working_out>.*?<SOLUTION>(.+?)</SOLUTION>[\\s]{0,}(?:<\\|eot_id\\|>)?[\\s]{0,}$',\n",
              "re.MULTILINE|re.DOTALL|re.UNICODE)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# Add optional EOS token matching\n",
        "solution_end_regex = r\"</SOLUTION>[\\s]{0,}\" + \\\n",
        "    \"(?:\" + re.escape(tokenizer.eos_token) + \")?\"\n",
        "\n",
        "match_format = re.compile(\n",
        "    rf\"{reasoning_end}.*?\"\\\n",
        "    rf\"{solution_start}(.+?){solution_end_regex}\"\\\n",
        "    rf\"[\\s]{{0,}}$\",\n",
        "    flags = re.MULTILINE | re.DOTALL\n",
        ")\n",
        "match_format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OycMneOq-iNC"
      },
      "source": [
        "We verify it works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndzHnQ_6-jHt",
        "outputId": "141a8575-89b5-4017-c2a3-c89a572f86aa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['\\n2\\n']"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "match_format.findall(\n",
        "    \"Let me think!<end_working_out>\"\\\n",
        "    f\"<SOLUTION>\\n2\\n</SOLUTION>\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRMDAzDk2x6t",
        "outputId": "4e1fde1a-473b-4868-c150-3cf7b309d1cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['  2  ']"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "match_format.findall(\n",
        "    \"<start_working_out>Let me think!<end_working_out>\"\\\n",
        "    f\"<SOLUTION>  2  </SOLUTION>\\n\\n\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weOjmO5l-kl3"
      },
      "source": [
        "We now want to create a reward function to match the format exactly - we reward it with 3 points if it succeeds:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "qgFNXORy-lpO"
      },
      "outputs": [],
      "source": [
        "def match_format_exactly(completions, **kwargs):\n",
        "    scores = []\n",
        "    for completion in completions:\n",
        "        score = 0\n",
        "        response = completion[0][\"content\"]\n",
        "        # Match if format is seen exactly!\n",
        "        if match_format.search(response) is not None: score += 3.0\n",
        "        scores.append(score)\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gf69i2WT-m4K"
      },
      "source": [
        "If it fails, we want to reward the model if it at least follows the format partially, by counting each symbol:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "cUfHzCVx-nGK"
      },
      "outputs": [],
      "source": [
        "def match_format_approximately(completions, **kwargs):\n",
        "    scores = []\n",
        "    for completion in completions:\n",
        "        score = 0\n",
        "        response = completion[0][\"content\"]\n",
        "        # Count how many keywords are seen - we penalize if too many!\n",
        "        # If we see 1, then plus some points!\n",
        "\n",
        "        # No need to reward <start_working_out> since we always prepend it!\n",
        "        # score += 0.5 if response.count(reasoning_start) == 1 else -1.0\n",
        "        score += 0.5 if response.count(reasoning_end)   == 1 else -1.0\n",
        "        score += 0.5 if response.count(solution_start)  == 1 else -1.0\n",
        "        score += 0.5 if response.count(solution_end)    == 1 else -1.0\n",
        "        scores.append(score)\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wAUWwtE-s6n"
      },
      "source": [
        "Finally, we want to extract the generated answer, and reward or penalize it! We also reward it based on how close the answer is to the true one via ratios:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "hmtI_8gg-uIE"
      },
      "outputs": [],
      "source": [
        "def check_answer(prompts, completions, answer, **kwargs):\n",
        "    question = prompts[0][-1][\"content\"]\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "\n",
        "    extracted_responses = [\n",
        "        guess.group(1)\n",
        "        if (guess := match_format.search(r)) is not None else None \\\n",
        "        for r in responses\n",
        "    ]\n",
        "\n",
        "    scores = []\n",
        "    for guess, true_answer in zip(extracted_responses, answer):\n",
        "        score = 0\n",
        "        if guess is None:\n",
        "            scores.append(-2.0)\n",
        "            continue\n",
        "        # Correct answer gets 5 points!\n",
        "        if guess == true_answer:\n",
        "            score += 5.0\n",
        "        # Match if spaces are seen, but less reward\n",
        "        elif guess.strip() == true_answer.strip():\n",
        "            score += 3.5\n",
        "        else:\n",
        "            # We also reward it if the answer is close via ratios!\n",
        "            # Ie if the answer is within some range, reward it!\n",
        "            try:\n",
        "                ratio = float(guess) / float(true_answer)\n",
        "                if   ratio >= 0.9 and ratio <= 1.1: score += 2.0\n",
        "                elif ratio >= 0.8 and ratio <= 1.2: score += 1.5\n",
        "                else: score -= 2.5 # Penalize wrong answers\n",
        "            except:\n",
        "                score -= 4.5 # Penalize\n",
        "        scores.append(score)\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atMyfhXh-v3R"
      },
      "source": [
        "Also sometimes it might not be 1 number as the answer, but like a sentence for example \"The solution is $20\" -> we extract 20.\n",
        "\n",
        "We also remove possible commas for example as in 123,456"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVW0kL8q-wL5",
        "outputId": "7c0f22d9-8c06-4e34-8096-4fa381075f83"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['0.34']\n",
            "['123,456']\n",
            "['-0.234']\n",
            "['17']\n"
          ]
        }
      ],
      "source": [
        "match_numbers = re.compile(\n",
        "    solution_start + r\".*?[\\s]{0,}([-]?[\\d\\.\\,]{1,})\",\n",
        "    flags = re.MULTILINE | re.DOTALL\n",
        ")\n",
        "print(match_numbers.findall(\"<SOLUTION>  0.34  </SOLUTION>\"))\n",
        "print(match_numbers.findall(\"<SOLUTION>  123,456  </SOLUTION>\"))\n",
        "print(match_numbers.findall(\"<SOLUTION>  -0.234  </SOLUTION>\"))\n",
        "print(match_numbers.findall(\"<SOLUTION>17</SOLUTION>\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbfaaAywNHHh"
      },
      "source": [
        "We now prepare our main function which will print out the generated responses and the true answer, along with another reward function which converts text to float via `float` and sees if it's the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "GjBFrttr-y1_"
      },
      "outputs": [],
      "source": [
        "global PRINTED_TIMES\n",
        "PRINTED_TIMES = 0\n",
        "global PRINT_EVERY_STEPS\n",
        "PRINT_EVERY_STEPS = 5\n",
        "\n",
        "def check_numbers(prompts, completions, answer, **kwargs):\n",
        "    question = prompts[0][-1][\"content\"]\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "\n",
        "    extracted_responses = [\n",
        "        guess.group(1)\n",
        "        if (guess := match_numbers.search(r)) is not None else None \\\n",
        "        for r in responses\n",
        "    ]\n",
        "\n",
        "    scores = []\n",
        "    # Print only every few steps\n",
        "    global PRINTED_TIMES\n",
        "    global PRINT_EVERY_STEPS\n",
        "    if PRINTED_TIMES % PRINT_EVERY_STEPS == 0:\n",
        "        print(\n",
        "            '*'*20 + f\"Question:\\n{question}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\"\n",
        "        )\n",
        "    PRINTED_TIMES += 1\n",
        "\n",
        "    for guess, true_answer in zip(extracted_responses, answer):\n",
        "        if guess is None:\n",
        "            scores.append(-2.5)\n",
        "            continue\n",
        "        # Convert to numbers\n",
        "        try:\n",
        "            true_answer = float(true_answer.strip())\n",
        "            # Remove commas like in 123,456\n",
        "            guess       = float(guess.strip().replace(\",\", \"\"))\n",
        "            scores.append(3.5 if guess == true_answer else -1.5)\n",
        "        except:\n",
        "            scores.append(0)\n",
        "            continue\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgOR3wJ_AyLr"
      },
      "source": [
        "Get the top 90% prompt length so we don't accidentally truncate them!\n",
        "\n",
        "Ie we'll remove the top 10% long prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EgAi4Q5fGE-",
        "outputId": "3458abd8-e2a5-463a-ecff-edf558fe56a0"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are given a problem.\n",
            "Think about the problem and provide your working out.\n",
            "Place it between <start_working_out> and <end_working_out>.\n",
            "Then, provide your solution between <SOLUTION></SOLUTION><|eot_id|>In triangle $ABC$, $\\sin \\angle A = \\frac{4}{5}$ and $\\angle A < 90^\\circ$. Let $D$ be a point outside triangle $ABC$ such that $\\angle BAD = \\angle DAC$ and $\\angle BDC = 90^\\circ$. Suppose that $AD = 1$ and that $\\frac{BD}{CD} = \\frac{3}{2}$. If $AB + AC$ can be expressed in the form $\\frac{a\\sqrt{b}}{c}$ where $a, b, c$ are pairwise relatively prime integers, find $a + b + c$.<start_working_out>\n",
            "Max Length =  198\n"
          ]
        }
      ],
      "source": [
        "tokenized = dataset.map(\n",
        "    lambda x: {\"tokens\" : tokenizer.apply_chat_template(x[\"prompt\"], add_generation_prompt = True, tokenize = True)},\n",
        "    batched = True,\n",
        ")\n",
        "print(tokenizer.decode(tokenized[0][\"tokens\"]))\n",
        "tokenized = tokenized.map(lambda x: {\"L\" : len(x[\"tokens\"])})\n",
        "\n",
        "import numpy as np\n",
        "maximum_length = int(np.quantile(tokenized[\"L\"], 0.9))\n",
        "print(\"Max Length = \", maximum_length)\n",
        "\n",
        "# Filter only samples smaller than 90% max length\n",
        "dataset = dataset.select(np.where(np.array(tokenized[\"L\"]) <= maximum_length)[0])\n",
        "del tokenized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-IOMhVg-2AM"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "\n",
        "Now set up GRPO Trainer and all configurations!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ptqkXK2D4d6p"
      },
      "outputs": [],
      "source": [
        "max_prompt_length = maximum_length + 1 # + 1 just in case!\n",
        "max_completion_length = max_seq_length - max_prompt_length\n",
        "\n",
        "from vllm import SamplingParams\n",
        "vllm_sampling_params = SamplingParams(\n",
        "    min_p = 0.1,\n",
        "    top_p = 1.0,\n",
        "    top_k = -1,\n",
        "    seed = 3407,\n",
        "    stop = [tokenizer.eos_token],\n",
        "    include_stop_str_in_output = True,\n",
        ")\n",
        "\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "training_args = GRPOConfig(\n",
        "    vllm_sampling_params = vllm_sampling_params,\n",
        "    temperature = 1.0,\n",
        "    learning_rate = 5e-6,\n",
        "    weight_decay = 0.001,\n",
        "    warmup_ratio = 0.1,\n",
        "    lr_scheduler_type = \"linear\",\n",
        "    # optim = \"adamw_8bit\",\n",
        "    optim=\"adamw_torch_fused\",\n",
        "    logging_steps = 1,\n",
        "    per_device_train_batch_size = 1,\n",
        "    gradient_accumulation_steps = 16, # Increase to 4 for smoother training\n",
        "    num_generations = 4, # Decrease if out of memory\n",
        "    max_prompt_length = max_prompt_length,\n",
        "    max_completion_length = max_completion_length,\n",
        "    # num_train_epochs = 1, # Set to 1 for a full training run\n",
        "    max_steps = 100,\n",
        "    save_steps = 100,\n",
        "    report_to = \"none\", # Can use Weights & Biases\n",
        "    output_dir = \"outputs\",\n",
        "\n",
        "    # For optional training + evaluation\n",
        "    # fp16_full_eval = True,\n",
        "    # per_device_eval_batch_size = 4,\n",
        "    # eval_accumulation_steps = 1,\n",
        "    # eval_strategy = \"steps\",\n",
        "    # eval_steps = 1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9Mv8UZO5hz-"
      },
      "source": [
        "And let's run the trainer! If you scroll up, you'll see a table of rewards. The goal is to see the `reward` column increase!\n",
        "\n",
        "You might have to wait 150 to 200 steps for any action. You'll probably get 0 reward for the first 100 steps. Please be patient!\n",
        "\n",
        "| Step | Training Loss | reward    | reward_std | completion_length | kl       |\n",
        "|------|---------------|-----------|------------|-------------------|----------|\n",
        "| 1    | 0.000000      | 0.125000  | 0.000000   | 200.000000        | 0.000000 |\n",
        "| 2    | 0.000000      | 0.072375  | 0.248112   | 200.000000        | 0.000000 |\n",
        "| 3    | 0.000000      | -0.079000 | 0.163776   | 182.500000        | 0.000005 |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "vzOuSVCL_GA9",
        "outputId": "fb41a5f7-fa48-4fe9-e405-26d720178630"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2259480327.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# For optional training + evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnew_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m trainer = GRPOTrainer(\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
          ]
        }
      ],
      "source": [
        "# For optional training + evaluation\n",
        "# new_dataset = dataset.train_test_split(test_size = 0.01)\n",
        "\n",
        "trainer = GRPOTrainer(\n",
        "    model = model,\n",
        "    processing_class = tokenizer,\n",
        "    reward_funcs = [\n",
        "        match_format_exactly,\n",
        "        match_format_approximately,\n",
        "        check_answer,\n",
        "        check_numbers,\n",
        "    ],\n",
        "    args = training_args,\n",
        "    # train_dataset = dataset,\n",
        "\n",
        "    # For optional training + evaluation\n",
        "    # train_dataset = new_dataset[\"train\"],\n",
        "    # eval_dataset = new_dataset[\"test\"],\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlaUdxC_VHpz"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Now let's try the model we just trained! First, let's first try the model without any GRPO trained:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtcz_lpbVC92"
      },
      "outputs": [],
      "source": [
        "text = \"What is the sqrt of 101?\"\n",
        "\n",
        "from vllm import SamplingParams\n",
        "sampling_params = SamplingParams(\n",
        "    temperature = 1.0,\n",
        "    top_k = 50,\n",
        "    max_tokens = 1024,\n",
        ")\n",
        "output = model.fast_generate(\n",
        "    [text],\n",
        "    sampling_params = sampling_params,\n",
        "    lora_request = None,\n",
        ")[0].outputs[0].text\n",
        "\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Colxz9TAVMsi"
      },
      "source": [
        "And now with the LoRA we just trained with GRPO - we first save the LoRA first!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AL-BcuB1VLIv"
      },
      "outputs": [],
      "source": [
        "model.save_lora(\"grpo_saved_lora\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4LMOBl8boGX"
      },
      "source": [
        "Verify LoRA is actually trained!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SfdI-ERbpiw"
      },
      "outputs": [],
      "source": [
        "from safetensors import safe_open\n",
        "\n",
        "tensors = {}\n",
        "with safe_open(\"grpo_saved_lora/adapter_model.safetensors\", framework = \"pt\") as f:\n",
        "    # Verify both A and B are non zero\n",
        "    for key in f.keys():\n",
        "        tensor = f.get_tensor(key)\n",
        "        n_zeros = (tensor == 0).sum() / tensor.numel()\n",
        "        assert(n_zeros.item() != tensor.numel())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwpbwnDBVRLg"
      },
      "source": [
        "Now we load the LoRA and test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zf_OY5WMVOxF"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\",   \"content\": \"What is the sqrt of 101?\"},\n",
        "]\n",
        "\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    tokenize = False,\n",
        ")\n",
        "from vllm import SamplingParams\n",
        "sampling_params = SamplingParams(\n",
        "    temperature = 1.0,\n",
        "    top_k = 50,\n",
        "    max_tokens = 2048,\n",
        ")\n",
        "output = model.fast_generate(\n",
        "    text,\n",
        "    sampling_params = sampling_params,\n",
        "    lora_request = model.load_lora(\"grpo_saved_lora\"),\n",
        ")[0].outputs[0].text\n",
        "\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aDgFfhFYIAS"
      },
      "source": [
        "Our reasoning model is much better - it's not always correct, since we only trained it for an hour or so - it'll be better if we extend the sequence length and train for longer!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NUEmHFSYNTp"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjXGTkp7YNtB"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False:\n",
        "    model.save_pretrained(\"model\")\n",
        "    tokenizer.save_pretrained(\"model\")\n",
        "if False:\n",
        "    model.push_to_hub(\"hf/model\", token = \"\")\n",
        "    tokenizer.push_to_hub(\"hf/model\", token = \"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52WMb3k_YPt8"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
        "\n",
        "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
        "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
        "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
        "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
        "\n",
        "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyEjW-WuYQIm"
      },
      "outputs": [],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
        "# And change hf to your username!\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
        "\n",
        "# Save to multiple GGUF options - much faster if you want multiple!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"hf/model\", # Change hf to your username!\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "        token = \"\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V15Yhj1V9lwG"
      },
      "source": [
        "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp.\n",
        "\n",
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ‚≠êÔ∏è <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠êÔ∏è\n",
        "</div>\n",
        "\n",
        "  This notebook and all Unsloth notebooks are licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5de75c9abdf34a7687cced18e1a61f61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_04405b9942e74d24b3fe31d771559a49",
              "IPY_MODEL_00919cc8d24e47ed966ff27b9f70a2e7",
              "IPY_MODEL_155dfa5fd685467687a1526891846a22"
            ],
            "layout": "IPY_MODEL_422624a5bdc54d98b543439bc096d941"
          }
        },
        "04405b9942e74d24b3fe31d771559a49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23607694d71145339e8aa231d48f63c0",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_10ff7194dd514aa094eab5041776882f",
            "value": ""
          }
        },
        "00919cc8d24e47ed966ff27b9f70a2e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ba0ba8a8f54484a9267a042d1c62146",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6b2b8be2edb349cd80e862429bcd5fe9",
            "value": 1
          }
        },
        "155dfa5fd685467687a1526891846a22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d102db560b904f2b938afafc880e40fb",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f63f09d55e8346b7a2644112358d7fa7",
            "value": "Loading‚Äásafetensors‚Äácheckpoint‚Äáshards:‚Äá100%‚ÄáCompleted‚Äá|‚Äá1/1‚Äá[00:08&lt;00:00,‚Äá‚Äá8.51s/it]\n"
          }
        },
        "422624a5bdc54d98b543439bc096d941": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23607694d71145339e8aa231d48f63c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10ff7194dd514aa094eab5041776882f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ba0ba8a8f54484a9267a042d1c62146": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b2b8be2edb349cd80e862429bcd5fe9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d102db560b904f2b938afafc880e40fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f63f09d55e8346b7a2644112358d7fa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bab2ffabb7d6482ca9495950f537619f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bf1145ec6de34f5cbc81199b3bd82fc0",
              "IPY_MODEL_6f67a5ea89e943deb0f64b0a0b52874b",
              "IPY_MODEL_9648dbd02fe04eb79d51ade0f9a95cca"
            ],
            "layout": "IPY_MODEL_a3cdc6577017430b838f8e3368ea29f8"
          }
        },
        "bf1145ec6de34f5cbc81199b3bd82fc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff9bc85eb2f647c7b060c140510d892b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a1f44ad66ecb4cb2b1f47993669f422b",
            "value": "Capturing‚ÄáCUDA‚Äágraph‚Äáshapes:‚Äá100%"
          }
        },
        "6f67a5ea89e943deb0f64b0a0b52874b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1d7ef98b84847c5a2ea0edbc85ab9a5",
            "max": 9,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ded0d4c78da6448f84e4cf49c65e2db7",
            "value": 9
          }
        },
        "9648dbd02fe04eb79d51ade0f9a95cca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8870da57aeb49379fc2ffd3ffbe3c13",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7861a0bed9e84c71934d7347ebb7a50b",
            "value": "‚Äá9/9‚Äá[00:01&lt;00:00,‚Äá‚Äá5.51it/s]"
          }
        },
        "a3cdc6577017430b838f8e3368ea29f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff9bc85eb2f647c7b060c140510d892b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1f44ad66ecb4cb2b1f47993669f422b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1d7ef98b84847c5a2ea0edbc85ab9a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ded0d4c78da6448f84e4cf49c65e2db7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e8870da57aeb49379fc2ffd3ffbe3c13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7861a0bed9e84c71934d7347ebb7a50b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8fd2ee6228fe44c9bf5d31b1058da778": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7cacb838c2f04b098fe19f26bfec54d1",
              "IPY_MODEL_6028501c29af4da3815fb2a52467a432",
              "IPY_MODEL_a709c0effc894e61bc59c10f9bfc8d79"
            ],
            "layout": "IPY_MODEL_4522ea35d20041888cd824bf90253d22"
          }
        },
        "7cacb838c2f04b098fe19f26bfec54d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_735479bf12ea4211befdec6a05f17c3e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0304e9d55f9c48aab8719294905cfdf0",
            "value": "Unsloth:‚ÄáTokenizing‚Äá[&quot;text&quot;]‚Äá(num_proc=6):‚Äá100%"
          }
        },
        "6028501c29af4da3815fb2a52467a432": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83b334b61b1e42d8882581991caa8559",
            "max": 78,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b1648d3ac6594e4d9d25e07d5a91a850",
            "value": 78
          }
        },
        "a709c0effc894e61bc59c10f9bfc8d79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_942bf492503a4a729a76b549b3a7278b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_138f549b6f7744b6beed345e06ebc81a",
            "value": "‚Äá78/78‚Äá[00:05&lt;00:00,‚Äá26.71‚Äáexamples/s]"
          }
        },
        "4522ea35d20041888cd824bf90253d22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "735479bf12ea4211befdec6a05f17c3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0304e9d55f9c48aab8719294905cfdf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "83b334b61b1e42d8882581991caa8559": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1648d3ac6594e4d9d25e07d5a91a850": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "942bf492503a4a729a76b549b3a7278b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "138f549b6f7744b6beed345e06ebc81a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}